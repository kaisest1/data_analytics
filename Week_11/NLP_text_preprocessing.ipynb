{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0f68ae5",
   "metadata": {},
   "source": [
    "# Natural Language Processing - Text Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2054b14d",
   "metadata": {},
   "source": [
    "## Libraries and settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ba4eb3dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory: /workspaces/data_analytics/Week_11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/vscode/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/vscode/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "# Libraries\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pprint import pprint\n",
    "\n",
    "import nltk\n",
    "\n",
    "# Import only once\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "from nltk.tag import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.chunk import tree2conlltags\n",
    "from nltk.chunk import conlltags2tree\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Current working directory\n",
    "print('Current working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e252684a",
   "metadata": {},
   "source": [
    "## Defining documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8057467",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Sun is the center of our solar system and provides light and heat to the planets. There are nine planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto. Earth is the only planet known to support life, with its diverse ecosystems and atmosphere.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Defining documents about the solar system\n",
    "d1 = 'The Sun is the center of our solar system and provides light and heat to the planets.'\n",
    "d2 = 'There are nine planets in the solar system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, Neptune and Pluto.'\n",
    "d3 = 'Earth is the only planet known to support life, with its diverse ecosystems and atmosphere.'\n",
    "\n",
    "corpus_01 = d1 + ' ' + d2 + ' ' + d3\n",
    "corpus_01"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49fadda5",
   "metadata": {},
   "source": [
    "## Text preprocessing\n",
    "#### Steps:\n",
    "- Text to lowercase\n",
    "- Removing punctuations\n",
    "- Tokenization\n",
    "- Removal of stop words\n",
    "- Lemmatization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e649b8",
   "metadata": {},
   "source": [
    "### Text to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2666c8b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sun is the center of our solar system and provides light and heat to the planets. there are nine planets in the solar system: mercury, venus, earth, mars, jupiter, saturn, uranus, neptune and pluto. earth is the only planet known to support life, with its diverse ecosystems and atmosphere.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Text to lowercase function\n",
    "def text_lowercase(text):\n",
    "    return text.lower()\n",
    "\n",
    "# Text to lowercase\n",
    "corpus_02 = text_lowercase(corpus_01)\n",
    "corpus_02"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c837286f",
   "metadata": {},
   "source": [
    "### Removing punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90067406",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the sun is the center of our solar system and provides light and heat to the planets there are nine planets in the solar system mercury venus earth mars jupiter saturn uranus neptune and pluto earth is the only planet known to support life with its diverse ecosystems and atmosphere'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove punctuation function\n",
    "def remove_punctuation(text):\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    return text.translate(translator)\n",
    "\n",
    "# Remove punctuation\n",
    "corpus_03 = remove_punctuation(corpus_02)\n",
    "corpus_03"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "986153d2",
   "metadata": {},
   "source": [
    "### Tokenize text & removal of stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2e99fbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of english stopwords:\n",
      "{\"you'd\", 'again', 'any', 'when', 'shouldn', 't', 'the', \"that'll\", 'yours', 'should', 'what', \"you're\", 'into', 'hers', 'theirs', 'here', 'won', 'whom', 'does', 'each', 'an', 'for', 'once', 'were', 'you', 'ours', 'to', 'such', 'mustn', 'will', 'now', 'by', 'do', 'most', \"didn't\", 'up', 'haven', 'while', 'out', 'or', 'with', 'o', 'same', \"won't\", 'during', \"hadn't\", 'him', 'further', 'he', 'are', 'don', 'mightn', 'it', \"aren't\", 'himself', \"doesn't\", 'll', 'above', 'about', \"weren't\", 'who', 'nor', 'how', \"isn't\", \"couldn't\", 'from', 'be', 'those', \"mustn't\", 'weren', 'they', \"it's\", 'had', 'so', 'she', \"she's\", 'itself', 'after', 'her', \"wasn't\", 'of', 'because', 'doesn', 'other', 'than', 'but', 'doing', 'through', 'am', 'ain', 'i', 'myself', 're', 'their', 'your', 'm', 'some', 'hadn', 'our', 'my', 'been', 'until', 'on', 'and', 'off', 'no', 'having', 'isn', 'aren', 'where', 'these', 'wouldn', 'this', 'couldn', 'under', 'both', 's', 'needn', 'shan', \"you've\", 'did', 'y', 'if', 'very', 've', 'was', 'few', 'being', 'over', 'a', 'before', 'against', 'me', 'its', 'yourself', \"needn't\", \"haven't\", 'too', 'ma', 'in', 'at', 'down', 'just', 'didn', 'that', 'why', \"wouldn't\", 'yourselves', 'has', \"shan't\", \"hasn't\", 'herself', 'there', \"you'll\", 'them', 'wasn', 'his', 'hasn', 'then', 'is', 'not', 'ourselves', \"mightn't\", 'have', 'below', 'more', 'can', 'd', 'we', 'only', 'which', \"should've\", 'as', 'between', 'themselves', 'all', 'own', \"shouldn't\", \"don't\"}\n"
     ]
    }
   ],
   "source": [
    "# Show english stopwords\n",
    "eng_stopwords = set(stopwords.words('english'))\n",
    "print(\"List of english stopwords:\")\n",
    "print(eng_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d83ab939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sun', 'center', 'solar', 'system', 'provides', 'light', 'heat', 'planets', 'nine', 'planets', 'solar', 'system', 'mercury', 'venus', 'earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune', 'pluto', 'earth', 'planet', 'known', 'support', 'life', 'diverse', 'ecosystems', 'atmosphere']"
     ]
    }
   ],
   "source": [
    "# Function for tokenization and the removal of stopwords\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return filtered_text\n",
    " \n",
    "# Remove stopwords\n",
    "corpus_04 = remove_stopwords(corpus_03)\n",
    "print(corpus_04, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad590183",
   "metadata": {},
   "source": [
    "### Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "410fed5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before lemmatization:\n",
      "['sun', 'center', 'solar', 'system', 'provides', 'light', 'heat', 'planets', 'nine', 'planets', 'solar', 'system', 'mercury', 'venus', 'earth', 'mars', 'jupiter', 'saturn', 'uranus', 'neptune', 'pluto', 'earth', 'planet', 'known', 'support', 'life', 'diverse', 'ecosystems', 'atmosphere'] \n",
      "\n",
      "After lemmatization:\n",
      "['sun', 'center', 'solar', 'system', 'provide', 'light', 'heat', 'planets', 'nine', 'planets', 'solar', 'system', 'mercury', 'venus', 'earth', 'mar', 'jupiter', 'saturn', 'uranus', 'neptune', 'pluto', 'earth', 'planet', 'know', 'support', 'life', 'diverse', 'ecosystems', 'atmosphere']"
     ]
    }
   ],
   "source": [
    "# Initialize Lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize string function\n",
    "def lemmatize_word(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    lemmas = [lemmatizer.lemmatize(word, pos ='v') for word in word_tokens]\n",
    "    return lemmas\n",
    "\n",
    "# Lemmatize\n",
    "lem = []\n",
    "for i in corpus_04:\n",
    "    lem.append(lemmatize_word(i))\n",
    "\n",
    "# Nested list to list\n",
    "corpus_05 = [' '.join([str(x) for x in lst]) for lst in lem]\n",
    "\n",
    "print('Before lemmatization:')\n",
    "print(corpus_04, '\\n')\n",
    "\n",
    "print('After lemmatization:')\n",
    "print(corpus_05, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d9ad6de",
   "metadata": {},
   "source": [
    "## Redefine the text corpus (pre-processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "08a3cb57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manuell angepasster Korpus mit lemmatisierten Wörtern\n",
    "corpus = ['sun center solar system provide light heat planet', \n",
    "          'nine planet solar system mercury venus earth mars jupiter saturn uranus neptune pluto', \n",
    "          'earth planet know support life diverse ecosystem atmosphere']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198cc6d0",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ead679d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   atmosphere  center  diverse  earth  ecosystem  heat  jupiter  know  life  \\\n",
      "0           0       1        0      0          0     1        0     0     0   \n",
      "1           0       0        0      1          0     0        1     0     0   \n",
      "2           1       0        1      1          1     0        0     1     1   \n",
      "\n",
      "   light  ...  planet  pluto  provide  saturn  solar  sun  support  system  \\\n",
      "0      1  ...       1      0        1       0      1    1        0       1   \n",
      "1      0  ...       1      1        0       1      1    0        0       1   \n",
      "2      0  ...       1      0        0       0      0    0        1       0   \n",
      "\n",
      "   uranus  venus  \n",
      "0       0      0  \n",
      "1       1      1  \n",
      "2       0      0  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with ngram_range=(1,1)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(1,1))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417feb3a",
   "metadata": {},
   "source": [
    "## Document-term matrix with ngram_range=(2,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4eb33ec6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document-term matrix\n",
      "   center solar  diverse ecosystem  earth mars  earth planet  \\\n",
      "0             1                  0           0             0   \n",
      "1             0                  0           1             0   \n",
      "2             0                  1           0             1   \n",
      "\n",
      "   ecosystem atmosphere  heat planet  jupiter saturn  know support  \\\n",
      "0                     0            1               0             0   \n",
      "1                     0            0               1             0   \n",
      "2                     1            0               0             1   \n",
      "\n",
      "   life diverse  light heat  ...  planet solar  provide light  saturn uranus  \\\n",
      "0             0           1  ...             0              1              0   \n",
      "1             0           0  ...             1              0              1   \n",
      "2             1           0  ...             0              0              0   \n",
      "\n",
      "   solar system  sun center  support life  system mercury  system provide  \\\n",
      "0             1           1             0               0               1   \n",
      "1             1           0             0               1               0   \n",
      "2             0           0             1               0               0   \n",
      "\n",
      "   uranus neptune  venus earth  \n",
      "0               0            0  \n",
      "1               1            1  \n",
      "2               0            0  \n",
      "\n",
      "[3 rows x 25 columns]\n"
     ]
    }
   ],
   "source": [
    "# Vectorizer with with ngram_range=(2,2)\n",
    "vectorizer = CountVectorizer(min_df=0.0, ngram_range=(2,2))\n",
    "\n",
    "# Transform \n",
    "count = vectorizer.fit_transform(corpus)\n",
    " \n",
    "# Create dataframe\n",
    "df_count = pd.DataFrame(count.toarray(),\n",
    "                        columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "print('Document-term matrix')\n",
    "print(df_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c846a236",
   "metadata": {},
   "source": [
    "## Term frequency-inverse document frequency (TF-IDF)\n",
    "- For details see: https://www.learndatasci.com/glossary/tf-idf-term-frequency-inverse-document-frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5854fa81",
   "metadata": {},
   "source": [
    "### Term Frequency (TF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f9ff38f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in the corpus: 24 \n",
      "\n",
      "The words in the corpus: \n",
      " {'life', 'heat', 'earth', 'ecosystem', 'atmosphere', 'venus', 'support', 'neptune', 'mercury', 'uranus', 'system', 'center', 'pluto', 'planet', 'nine', 'light', 'diverse', 'provide', 'solar', 'know', 'sun', 'saturn', 'mars', 'jupiter'}\n",
      "\n",
      "Term Frequency (TF):\n",
      "    life   heat   earth  ecosystem  atmosphere   venus  support  neptune  \\\n",
      "0  0.000  0.125  0.0000      0.000       0.000  0.0000    0.000   0.0000   \n",
      "1  0.000  0.000  0.0769      0.000       0.000  0.0769    0.000   0.0769   \n",
      "2  0.125  0.000  0.1250      0.125       0.125  0.0000    0.125   0.0000   \n",
      "\n",
      "   mercury  uranus  ...    nine  light  diverse  provide   solar   know  \\\n",
      "0   0.0000  0.0000  ...  0.0000  0.125    0.000    0.125  0.1250  0.000   \n",
      "1   0.0769  0.0769  ...  0.0769  0.000    0.000    0.000  0.0769  0.000   \n",
      "2   0.0000  0.0000  ...  0.0000  0.000    0.125    0.000  0.0000  0.125   \n",
      "\n",
      "     sun  saturn    mars  jupiter  \n",
      "0  0.125  0.0000  0.0000   0.0000  \n",
      "1  0.000  0.0769  0.0769   0.0769  \n",
      "2  0.000  0.0000  0.0000   0.0000  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Compute Term Frequency (TF)\n",
    "words_set = set()\n",
    "for doc in corpus:\n",
    "    words = doc.split(' ')\n",
    "    words_set = words_set.union(set(words))\n",
    "    \n",
    "print('Number of words in the corpus:',len(words_set), '\\n')\n",
    "print('The words in the corpus: \\n', words_set)\n",
    "\n",
    "# Number of documents in the corpus\n",
    "n_docs = len(corpus)\n",
    "\n",
    "# Number of unique words in the corpus \n",
    "n_words_set = len(words_set)\n",
    "\n",
    "df_tf = pd.DataFrame(np.zeros((n_docs, n_words_set)), \n",
    "                     columns=list(words_set))\n",
    "\n",
    "print(\"\\nTerm Frequency (TF):\")\n",
    "for i in range(n_docs):\n",
    "    # Words in the document\n",
    "    words = corpus[i].split(' ')\n",
    "    for w in words:\n",
    "        df_tf[w][i] = df_tf[w][i] + (1 / len(words))\n",
    "        \n",
    "print(df_tf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91dae3d",
   "metadata": {},
   "source": [
    "### Inverse Document Frequency (IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5fe31336",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inverse Document Frequency (IDF):\n",
      "           life:     0.4771\n",
      "           heat:     0.4771\n",
      "          earth:     0.1761\n",
      "      ecosystem:     0.4771\n",
      "     atmosphere:     0.4771\n",
      "          venus:     0.4771\n",
      "        support:     0.4771\n",
      "        neptune:     0.4771\n",
      "        mercury:     0.4771\n",
      "         uranus:     0.4771\n",
      "         system:     0.1761\n",
      "         center:     0.4771\n",
      "          pluto:     0.4771\n",
      "         planet:        0.0\n",
      "           nine:     0.4771\n",
      "          light:     0.4771\n",
      "        diverse:     0.4771\n",
      "        provide:     0.4771\n",
      "          solar:     0.1761\n",
      "           know:     0.4771\n",
      "            sun:     0.4771\n",
      "         saturn:     0.4771\n",
      "           mars:     0.4771\n",
      "        jupiter:     0.4771\n"
     ]
    }
   ],
   "source": [
    "# Computing Inverse Document Frequency (IDF)\n",
    "print(\"\\nInverse Document Frequency (IDF):\")\n",
    "\n",
    "idf = {}\n",
    "\n",
    "for w in words_set:\n",
    "    \n",
    "    # k = number of documents that contain this word\n",
    "    k = 0\n",
    "    \n",
    "    for i in range(n_docs):\n",
    "        if w in corpus[i].split():\n",
    "            k += 1\n",
    "            \n",
    "    idf[w] =  np.log10(n_docs / k).round(4)\n",
    "    \n",
    "    print(f'{w:>15}: {idf[w]:>10}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc493eae",
   "metadata": {},
   "source": [
    "### Term Frequency - Inverse Document Frequency (TF-IDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9c5ae575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF:\n",
      "     life    heat   earth  ecosystem  atmosphere   venus  support  neptune  \\\n",
      "0  0.0000  0.0596  0.0000     0.0000      0.0000  0.0000   0.0000   0.0000   \n",
      "1  0.0000  0.0000  0.0135     0.0000      0.0000  0.0367   0.0000   0.0367   \n",
      "2  0.0596  0.0000  0.0220     0.0596      0.0596  0.0000   0.0596   0.0000   \n",
      "\n",
      "   mercury  uranus  ...    nine   light  diverse  provide   solar    know  \\\n",
      "0   0.0000  0.0000  ...  0.0000  0.0596   0.0000   0.0596  0.0220  0.0000   \n",
      "1   0.0367  0.0367  ...  0.0367  0.0000   0.0000   0.0000  0.0135  0.0000   \n",
      "2   0.0000  0.0000  ...  0.0000  0.0000   0.0596   0.0000  0.0000  0.0596   \n",
      "\n",
      "      sun  saturn    mars  jupiter  \n",
      "0  0.0596  0.0000  0.0000   0.0000  \n",
      "1  0.0000  0.0367  0.0367   0.0367  \n",
      "2  0.0000  0.0000  0.0000   0.0000  \n",
      "\n",
      "[3 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Computing TF-IDF\n",
    "df_tf_idf = df_tf.copy()\n",
    "\n",
    "for w in words_set:\n",
    "    for i in range(n_docs):\n",
    "        df_tf_idf[w][i] = df_tf[w][i] * idf[w]\n",
    "\n",
    "print('\\nTF-IDF:')\n",
    "print(df_tf_idf.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7b0f38",
   "metadata": {},
   "source": [
    "## Part-of-Speach (POS) tagging\n",
    "For meaning of POS-tags see: https://pythonexamples.org/nltk-pos-tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5c8c05c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('The', 'DT', 'O'),\n",
      " ('Sun', 'NNP', 'O'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('center', 'NN', 'I-NP'),\n",
      " ('of', 'IN', 'O'),\n",
      " ('our', 'PRP$', 'O'),\n",
      " ('solar', 'JJ', 'B-NP'),\n",
      " ('system', 'NN', 'I-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('provides', 'VBZ', 'O'),\n",
      " ('light', 'NN', 'B-NP'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('heat', 'NN', 'B-NP'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('the', 'DT', 'O'),\n",
      " ('planets', 'NNS', 'O'),\n",
      " ('.', '.', 'O'),\n",
      " ('There', 'EX', 'O'),\n",
      " ('are', 'VBP', 'O'),\n",
      " ('nine', 'CD', 'O'),\n",
      " ('planets', 'NNS', 'O'),\n",
      " ('in', 'IN', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('solar', 'JJ', 'I-NP'),\n",
      " ('system', 'NN', 'I-NP'),\n",
      " (':', ':', 'O'),\n",
      " ('Mercury', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Venus', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Earth', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Mars', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Jupiter', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Saturn', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Uranus', 'NNP', 'O'),\n",
      " (',', ',', 'O'),\n",
      " ('Neptune', 'NNP', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('Pluto', 'NNP', 'O'),\n",
      " ('.', '.', 'O'),\n",
      " ('Earth', 'NN', 'B-NP'),\n",
      " ('is', 'VBZ', 'O'),\n",
      " ('the', 'DT', 'B-NP'),\n",
      " ('only', 'JJ', 'I-NP'),\n",
      " ('planet', 'NN', 'I-NP'),\n",
      " ('known', 'VBN', 'O'),\n",
      " ('to', 'TO', 'O'),\n",
      " ('support', 'VB', 'O'),\n",
      " ('life', 'NN', 'B-NP'),\n",
      " (',', ',', 'O'),\n",
      " ('with', 'IN', 'O'),\n",
      " ('its', 'PRP$', 'O'),\n",
      " ('diverse', 'NN', 'B-NP'),\n",
      " ('ecosystems', 'NNS', 'O'),\n",
      " ('and', 'CC', 'O'),\n",
      " ('atmosphere', 'RB', 'O'),\n",
      " ('.', '.', 'O')]\n"
     ]
    }
   ],
   "source": [
    "text = '''The Sun is the center of our solar system and provides \n",
    "light and heat to the planets. There are nine planets in the solar \n",
    "system: Mercury, Venus, Earth, Mars, Jupiter, Saturn, Uranus, \n",
    "Neptune and Pluto. Earth is the only planet known to support life, \n",
    "with its diverse ecosystems and atmosphere.'''\n",
    "\n",
    "def preprocess(sent):\n",
    "    sent = nltk.word_tokenize(sent)\n",
    "    sent = nltk.pos_tag(sent)\n",
    "    return sent\n",
    "\n",
    "sent = preprocess(text)\n",
    "pattern = 'NP: {<DT>?<JJ>*<NN>}'\n",
    "\n",
    "cp = nltk.RegexpParser(pattern)\n",
    "cs = cp.parse(sent)\n",
    "\n",
    "iob_tagged = tree2conlltags(cs)\n",
    "\n",
    "# Print the POS-tags\n",
    "pprint(iob_tagged)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c395146",
   "metadata": {},
   "source": [
    "### f)In the section ‘Part-of-Speach (POS) tagging’, choose your own text example and derive POS tags for this example. In the Jupyter notebook, briefly explain the meaning of at least 5 POS-tags from the output of the POS tagging.\n",
    "\n",
    "('The', 'DT', 'O') DT (Determiner): The word \"The\" is tagged as a determiner. \n",
    "\n",
    "('Sun', 'NNP', 'O') NNP (Proper Noun, Singular): \"Sun\" is tagged as a proper noun in singular form.\n",
    "\n",
    "('is', 'VBZ', 'O') VBZ (Verb, 3rd Person Singular Present): The word \"is\" is tagged as a present-tense verb for the 3rd person singular\n",
    "\n",
    "('center', 'NN', 'I-NP') NN (Noun, Singular): \"Center\" is tagged as a singular noun, indicating a thing or place, in this case, the center of something.\n",
    "\n",
    "('of', 'IN', 'O') IN (Preposition/Subordinating Conjunction): The word \"of\" is tagged as a preposition\n",
    "\n",
    "'O' (Outside): \"of\" is outside any named entity or chunked phrase here.\n",
    "\n",
    "'I-NP' (Inside Noun Phrase): This tag shows that \"center\" is inside a noun phrase and follows the determiner \"the,\" continuing the noun phrase that started with \"the.\"\n",
    "\n",
    "'B-NP' (Beginning of Noun Phrase): This indicates the beginning of a noun phrase (NP), with \"the\" as the first word of this chunked phrase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d1243de",
   "metadata": {},
   "source": [
    "### Jupyter notebook --footer info-- (please always provide this at the end of each submitted notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "017357b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------\n",
      "POSIX\n",
      "Linux | 6.5.0-1025-azure\n",
      "Datetime: 2024-10-30 16:42:06\n",
      "Python Version: 3.11.10\n",
      "-----------------------------------\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import platform\n",
    "import socket\n",
    "from platform import python_version\n",
    "from datetime import datetime\n",
    "\n",
    "print('-----------------------------------')\n",
    "print(os.name.upper())\n",
    "print(platform.system(), '|', platform.release())\n",
    "print('Datetime:', datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"))\n",
    "print('Python Version:', python_version())\n",
    "print('-----------------------------------')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
